{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "020ec0b6",
   "metadata": {},
   "source": [
    "<h3><b>1. Scrapping Data 50x3 Artikel</b></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f486b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries (if not already installed)\n",
    "!pip install beautifulsoup4 requests pandas -q\n",
    "!pip install openpyxl -q\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Read URLs from an Excel file (replace 'urls.xlsx' with your file path)\n",
    "excel_path = 'data/artikel_damri.xlsx'  \n",
    "df_urls = pd.read_excel(excel_path, engine='openpyxl')\n",
    "\n",
    "# If the sheet has a column named 'url' use it, otherwise use the first column\n",
    "if 'link' in [c.lower() for c in df_urls.columns]:\n",
    "    # find the actual column name matching 'url' (case-insensitive)\n",
    "    url_col = next(c for c in df_urls.columns if c.lower() == 'link')\n",
    "    urls = df_urls[url_col].dropna().astype(str).tolist()\n",
    "else:\n",
    "    urls = df_urls.iloc[:, 0].dropna().astype(str).tolist()\n",
    "\n",
    "# Function to scrape content from a URL\n",
    "def scrape_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Extract the title of the article\n",
    "            title = soup.find('title').text\n",
    "\n",
    "            # Extract the article's content\n",
    "            paragraphs = soup.find_all('p')\n",
    "            content = \"\\n\".join([para.text for para in paragraphs])\n",
    "\n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"title\": title,\n",
    "                \"content\": content\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"title\": None,\n",
    "                \"content\": None,\n",
    "                \"error\": f\"Failed to fetch page, status code: {response.status_code}\"\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"title\": None,\n",
    "            \"content\": None,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Scrape each URL and store the results in a list of dictionaries\n",
    "data = []\n",
    "for url in urls:\n",
    "    result = scrape_content(url)\n",
    "    data.append(result)\n",
    "\n",
    "# Create a pandas DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()\n",
    "\n",
    "# Save the DataFrame to a CSV file (optional)\n",
    "df.to_csv('data/scraped_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd8b1fe",
   "metadata": {},
   "source": [
    "<h3><b>2. Lakukan EDA untuk Dataset Scraping</b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167b3605",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas matplotlib seaborn nltk\n",
    "!pip install Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788dc822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "# Create stemmer and lemmatizer objects\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Case lowering\n",
    "    text = text.lower()\n",
    "\n",
    "    # Punctuation removal\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Stemming\n",
    "    text = stemmer.stem(text)\n",
    "\n",
    "    # Lemmatization (requires a part-of-speech tag for optimal results, using 'v' for verb as a default)\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, 'v') for word in words]\n",
    "    text = ' '.join(lemmatized_words)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'review' column\n",
    "df['cleaned_review'] = df['content'].apply(clean_text)\n",
    "\n",
    "# Display the DataFrame with the cleaned reviews\n",
    "print(df_jknrev[['review', 'cleaned_review']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b558e231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
